<!doctype html>



  


<html class="theme-next muse use-motion">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  




<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css">


  <meta name="keywords" content="caffe,">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1">






<meta name="description" content="blob.hpp &amp;amp;&amp;amp; blob.cpp">
<meta name="keywords" content="caffe">
<meta property="og:type" content="article">
<meta property="og:title" content="Blob.hpp和cpp文件解读">
<meta property="og:url" content="http://magic93.cn/2016/08/30/Blob-hpp和cpp文件解读/index.html">
<meta property="og:site_name" content="Alex的博客">
<meta property="og:description" content="blob.hpp &amp;amp;&amp;amp; blob.cpp">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2022-10-10T03:50:42.615Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Blob.hpp和cpp文件解读">
<meta name="twitter:description" content="blob.hpp &amp;amp;&amp;amp; blob.cpp">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>

  <title> Blob.hpp和cpp文件解读 | Alex的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang>

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Alex的博客</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th fa-fw"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-user fa-fw"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br>
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Blob.hpp和cpp文件解读
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-08-30T17:05:01+08:00" content="2016-08-30">
              2016-08-30
            </time>
          </span>

          
            <span class="post-category">
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/08/30/Blob-hpp和cpp文件解读/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/08/30/Blob-hpp和cpp文件解读/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/08/30/Blob-hpp和cpp文件解读/" class="leancloud_visitors" data-flag-title="Blob.hpp和cpp文件解读">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><blockquote class="blockquote-center">blob.hpp &amp;&amp; blob.cpp</blockquote><br><a id="more"></a></p>
<p>#ifndef CAFFE_BLOB_HPP_//防止头文件重复引用  </p>
<pre><code>#define CAFFE_BLOB_HPP_  

#include &lt;algorithm&gt;  
#include &lt;string&gt;  
#include &lt;vector&gt;  

/*common.hpp主要用来单例化Caffe类， 
*并封装了boost和CUDA随机数生成的函数， 
*提供了统一的接口 
*/  
#include &quot;common.hpp&quot;  
/*caffe.pb.h是google protocol buffer根据caffe.proto自动生成的。 
*使用protocol buffer有这些好处，一方面可以用文本文件定义结构化的数据类型， 
*另一方面可以生成查询效率更高、占空间更小的二进制文件 
*/  
#include &quot;caffe.pb.h&quot;  
//syncedmem主要用于分配内存和释放内存  
#include &quot;syncedmem.hpp&quot;  
//math_functions里面封装了很多cblas矩阵运算  
#include &quot;util/math_functions.hpp&quot;  

const int kMaxBlobAxes = INT_MAX;  

namespace caffe {//命名空间为caffe  

    /* 
    *主要数据有两个data和diff，用num、channels、height和width 
    *这四个维度来确定数据的具体位置，做一些数据查询和Blobreshape的操作 
    */  
    template &lt;typename Dtype&gt;  
    class Blob {  
    public:  
        Blob()//blob的构造函数  
            : data_(), diff_(), count_(0), capacity_(0)//数据容量 {}
            //data_(), diff_()是用于存放数据的指针，  
        /*num_, channel_, height_, width_主要用来做定位offset和reshape处理。 
        *对于输入(n, c, h, w)位置的数据位置为((n*channels_+c)*height_+h)*width_+w，*可以依据位置取data_()或diff_()中的数据。 
        */  
        explicit Blob(const int num, const int channels, const int height, const int width);  
        explicit Blob(const vector&lt;int&gt;&amp; shape);  

        /*Reshape函数的作用是改变一个blob的大小 
        *1.读入num_，channels_，height_，width_的大小  
        *2.计算count_：count_ = num_ * channels_ * height_ * width_;
        *3.如果count_不为0，则重新为data_和diff_分配一块空间  
        *如果count为0，则都初始化为NULL 
        */  
        void Reshape(const int num, const int channels, const int height,  const int width);  
        void Reshape(const vector&lt;int&gt;&amp; shape);  
        void Reshape(const BlobShape&amp; shape);  
        //ReshapeLike的作用是为data_和diff_ 重新分配一块空间，大小和另一个blob的一样   
        void ReshapeLike(const Blob&amp; other);  

        inline string shape_string() const {  
            ostringstream stream;  //strem是一个string流
            for (int i = 0; i &lt; shape_.size(); ++i) {  
                stream &lt;&lt; shape_[i] &lt;&lt; &quot; &quot;;  //向流中传递shape_数据
            }  
            stream &lt;&lt; &quot;(&quot; &lt;&lt; count_ &lt;&lt; &quot;)&quot;;  //传递数据大小（四维相乘）
            return stream.str();  //返回字符串格式
        }  
        inline const vector&lt;int&gt;&amp; shape() const { return shape_; }//返回shape  

        //返回第i个索引的shape,index可以是负数，  
        inline int shape(int index) const {  
            return shape_[CanonicalAxisIndex(index)];  //为了可以是负数
        }  
        inline int num_axes() const { return shape_.size(); }//返回shape的大小  
        inline int count() const { return count_; }//返回参数count  

        //计算一个slice的体积  
        ////为了统计Blob的容量（volume），或者是某一片（slice），从某个axis到具体某个axis的shape乘积。
        inline int count(int start_axis, int end_axis) const {  
        CHECK_LE(start_axis, end_axis);  
        CHECK_GE(start_axis, 0);  
        CHECK_GE(end_axis, 0);  
        CHECK_LE(start_axis, num_axes());  
        CHECK_LE(end_axis, num_axes()); 
            int count = 1;  
            for (int i = start_axis; i &lt; end_axis; ++i) {  
                count *= shape(i);  //num_,channel_,height_,width_都可以直接通过shape(i)访问
            }  
            return count;  
        }  
        //计算从从一个特定的axis到最后一个axis的slice的体积。 
        inline int count(int start_axis) const {  
            return count(start_axis, num_axes());  
        }  

        //Blob的Index是可以从负坐标开始读的,标准化索引，主要是对参数索引进行标准化，以满足要求  
  inline int CanonicalAxisIndex(int axis_index) const {  
    CHECK_GE(axis_index, -num_axes())  
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()  
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();  
    CHECK_LT(axis_index, num_axes())  
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()  
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();  
    if (axis_index &lt; 0) {  
      return axis_index + num_axes();  
    }  
    return axis_index;  
  }  
  //Blob中的4个基本变量num,channel,height,width可以直接通过shape(0),shape(1),shape(2),shape(3)来访问  
  /// @brief Deprecated legacy shape accessor num: use shape(0) instead.  
  inline int num() const { return LegacyShape(0); }  
  /// @brief Deprecated legacy shape accessor channels: use shape(1) instead.  
  inline int channels() const { return LegacyShape(1); }  
  /// @brief Deprecated legacy shape accessor height: use shape(2) instead.  
  inline int height() const { return LegacyShape(2); }  
  /// @brief Deprecated legacy shape accessor width: use shape(3) instead.  
  inline int width() const { return LegacyShape(3); }  
//data_维数不大于4时才能使用，功能同shape()类似。  
  inline int LegacyShape(int index) const {  
    CHECK_LE(num_axes(), 4)  
        &lt;&lt; &quot;Cannot use legacy accessors on Blobs with &gt; 4 axes.&quot;;  
    CHECK_LT(index, 4);  //lower than判断是否小于
    CHECK_GE(index, -4);  
    if (index &gt;= num_axes() || index &lt; -num_axes()) {  
      // Axis is out of range, but still in [0, 3] (or [-4, -1] for reverse  
      // indexing) -- this special case simulates the one-padding used to fill  
      // extraneous axes of legacy blobs.  
      return 1;  
    }  
    return shape(index);  
  }  

  //计算offset,offset计算的方式也支持两种方式，一种直接指定n,c,h,w或者放到一个vector中进行计算，  
  //偏差是根据对应的n,c,h,w，返回的offset是((n*channels()+c)*height()+h)*width()+w  
  inline int offset(const int n, const int c = 0, const int h = 0,  
      const int w = 0) const {  
    CHECK_GE(n, 0);   //greater or equal，判断是否大于等于
    CHECK_LE(n, num());  //lower or equal ，判断是否小于等于
    CHECK_GE(channels(), 0);  
    CHECK_LE(c, channels());  
    CHECK_GE(height(), 0);  
    CHECK_LE(h, height());  
    CHECK_GE(width(), 0);  
    CHECK_LE(w, width());  
    return ((n * channels() + c) * height() + h) * width() + w;  
  }  

  inline int offset(const vector&lt;int&gt;&amp; indices) const {  
    CHECK_LE(indices.size(), num_axes());  
    int offset = 0;  
    for (int i = 0; i &lt; num_axes(); ++i) {  
      offset *= shape(i);  
      if (indices.size() &gt; i) {  
        CHECK_GE(indices[i], 0);  
        CHECK_LT(indices[i], shape(i));  
        offset += indices[i];  
      }  
    }  
    return offset;  
  }  
/** 
 *从source拷贝数据。copy_diff作为标志来区分是拷贝data还是拷贝diff 
            *1.如果是GPU： 如果是拷贝diff：调用cudaMemcpy函数将source的diff拷贝过来，否则拷贝data  
            *2.如果是CPU： 如果是拷贝diff：调用memcpy函数将source的diff拷贝过来 否则拷贝data 
            */  
    void CopyFrom(const Blob&lt;Dtype&gt;&amp; source, bool copy_diff = false,bool reshape = false);  
    //从cpu访问数据data  
inline Dtype data_at(const int n, const int c, const int h, const int w) const {  
            return cpu_data()[offset(n, c, h, w)];  
            }  
            //从cpu访问数据diff  
 inline Dtype diff_at(const int n, const int c, const int h, const int w) const {  
            return cpu_diff()[offset(n, c, h, w)];  
            }  
            //从cpu访问数据data  
inline Dtype data_at(const vector&lt;int&gt;&amp; index) const {  
            return cpu_data()[offset(index)];  
            }  
            //从cpu访问数据diff  
 inline Dtype diff_at(const vector&lt;int&gt;&amp; index) const {  
                return cpu_diff()[offset(index)];  
            }  
            //从cpu访问数据data  
 inline const shared_ptr&lt;SyncedMemory&gt;&amp; data() const {  
                return data_;  
            }  
            //从cpu访问数据diff  
inline const shared_ptr&lt;SyncedMemory&gt;&amp; diff() const {  
                return diff_;  
            }  
 /**调用SyncedMemory的函数，来返回数据的指针;前两个调用to_cpu(),返回cpu_ptr； 
        *第一个对于data对象，第二个对于diff对象 
        *后两个调用to_gpu(),返回gpu_ptr；第一个对于data对象，第二个对于diff对象 
           */  
        void set_cpu_data(Dtype* data);  
        const Dtype* cpu_data() const;  
        const Dtype* gpu_data() const;  
        const Dtype* cpu_diff() const;  
        const Dtype* gpu_diff() const;  
        Dtype* mutable_cpu_data();  
        Dtype* mutable_gpu_data();  
        Dtype* mutable_cpu_diff();  
        Dtype* mutable_gpu_diff();  
    /**更新data_的数据，就是减去diff_的数据。  
       *1.判断blob的位置 
    *2.调用caffe_axpy：在math_functions.cpp可以找到该函数的实现，其实这函数也是封装了mkl的函数。这里调用是为了实现了两个向量的减法。  
    *3.调用caffe_gpu_axpy：在math_functions.cpp可以找到该函数的实现，其实这函数也是封装了cublas的函数。这里调用是为了实现了两个向量的减法。 
       */  
        void Update();  
        /**功能：从proto读数据进来，其实就是反序列化  
        *1.先把blob的大小改变一下  
        *2.得到cpu中数据的地址  
        *3.用proto中的data覆盖blob中的data  
        *4.用proto中的diff覆盖blob中的diff 
        */  
        void FromProto(const BlobProto&amp; proto, bool reshape = true);  
        //把blob数据保存到proto中  
        void ToProto(BlobProto* proto, bool write_diff = false) const;  

        //计算绝对值的data总和（L1范数）。  
        Dtype asum_data() const;  
        //计算绝对值的diff总和（L1范数）。  
        Dtype asum_diff() const;  
        //计算绝对值的data总和（L2范数）。  
        Dtype sumsq_data() const;  
        //计算绝对值的diff总和（L2范数）。  
        Dtype sumsq_diff() const;  
        //通过常量因子缩放blob data  
        void scale_data(Dtype scale_factor);  
        ////通过常量因子缩放blob diff  
        void scale_diff(Dtype scale_factor);  

        //从other的blob复制data和diff的值  
        void ShareData(const Blob&amp; other);  
        void ShareDiff(const Blob&amp; other);  
        bool ShapeEquals(const BlobProto&amp; other);  

    protected:  
        shared_ptr&lt;SyncedMemory&gt; data_;// 存放数据  
        shared_ptr&lt;SyncedMemory&gt; diff_;//存放梯度  
        vector&lt;int&gt; shape_;//存放形状  
        int count_;//数据个数  4维相乘
        int capacity_;//数据容量  

        DISABLE_COPY_AND_ASSIGN(Blob);  
    };  // class Blob  

}  // namespace caffe  

#endif  // CAFFE_BLOB_HPP_  
</code></pre><p>首先是 data_ 指针，指针类型是shared_ptr，属于boost库的一个智能指针，这一部分主要用来申请内存存储data，data主要是正向传播的时候用的。同理， diff_ 主要用来存储偏差，update data， shape_data 和 shape_ 都是存储Blob的形状，一个是老版本一个是新版本。 count 表示Blob中的元素个数，也就是 个数<em>通道数</em>高度*宽度 , capacity 表示当前的元素个数，因为Blob可能会reshape。<br>Blob类里面有重载很多个 count()函数，主要还是为了统计Blob的容量（volume），或者是某一片（slice），从某个axis到具体某个axis的shape乘积。</p>
<pre><code>void FromProto(const BlobProto&amp; proto, bool reshape = true);
void ToProto(BlobProto* proto, bool write_diff = false) const;
</code></pre><p>这两个函数主要是将数据序列化，存储到BlobProto，这里说到Proto是谷歌的一个数据序列化的存储格式，可以实现语言、平台无关、可扩展的序列化结构数据格式。Caffe里面数据的存储都采用这一结构，这里就不深入展开，具体可以参照这篇文章，对于proto的序列化和反序列都讲解的非常详细 <a href="http://www.w2bc.com/Article/34963" target="_blank" rel="noopener">参考</a><br>  //把blob数据保存到proto中<br>            void ToProto(BlobProto* proto, bool write_diff = false) const;</p>
<p>对任何Blob<dtype> blobs调用的都是这个类，成员变量就包括<br>shared_ptr<syncedmemory> data_;// 存放数据<br>            shared_ptr<syncedmemory> diff_;//存放梯度<br>            vector<int> shape_;//存放形状<br>            int count_;//数据个数  4维相乘<br>            int capacity_;//数据容量 </int></syncedmemory></syncedmemory></dtype></p>
<pre><code>#include &lt;climits&gt;  
#include &lt;vector&gt;  

#include &quot;caffe/blob.hpp&quot;  
#include &quot;caffe/common.hpp&quot;  
#include &quot;caffe/syncedmem.hpp&quot;  
#include &quot;caffe/util/math_functions.hpp&quot;  

namespace caffe {  

template &lt;typename Dtype&gt;  
//该函数将num,channels,height,width传递给vector shape_   
void Blob&lt;Dtype&gt;::Reshape(const int num, const int channels, const int height,  
    const int width) {  
  vector&lt;int&gt; shape(4);  
  shape[0] = num;  
  shape[1] = channels;  
  shape[2] = height;  
  shape[3] = width;  
  Reshape(shape);  
}  

template &lt;typename Dtype&gt;  
void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp; shape) {  
  CHECK_LE(shape.size(), kMaxBlobAxes);  
  count_ = 1;  
  shape_.resize(shape.size());//重新定义vector shape_ 的size  
  for (int i = 0; i &lt; shape.size(); ++i) {  
    CHECK_GE(shape[i], 0);//确保shape 每个元素为正数  
    CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; &quot;blob size exceeds INT_MAX&quot;;  
    count_ *= shape[i];  
    shape_[i] = shape[i];  
  }  
  //由于count_超过了当前capacity_ 因此需要重新分配内存空间  
  if (count_ &gt; capacity_) {  
    capacity_ = count_;  
    data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));  
    diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));  
  }  
}  

template &lt;typename Dtype&gt;// BlobShape 在caffe.proto 中定义  
void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape) {  
  CHECK_LE(shape.dim_size(), kMaxBlobAxes);  
  vector&lt;int&gt; shape_vec(shape.dim_size());  
  for (int i = 0; i &lt; shape.dim_size(); ++i) {  
    shape_vec[i] = shape.dim(i);//dim 包含num，channels，height， width  
  }  
  Reshape(shape_vec);//用protobuf传递来dim 对shape_ 进行reshape  
}  
//用已知的Blob的shape来对shape_ 进行reshape  
template &lt;typename Dtype&gt;  
void Blob&lt;Dtype&gt;::ReshapeLike(const Blob&lt;Dtype&gt;&amp; other) {  
  Reshape(other.shape());  
}  
//用num，channels，height， width 初始化  
template &lt;typename Dtype&gt;  
Blob&lt;Dtype&gt;::Blob(const int num, const int channels, const int height,  
    const int width)  
  // capacity_ must be initialized before calling Reshape  
  : capacity_(0) {  
  Reshape(num, channels, height, width);  
}  
//用shape 初始化  
template &lt;typename Dtype&gt;  
Blob&lt;Dtype&gt;::Blob(const vector&lt;int&gt;&amp; shape)  
  // capacity_ must be initialized before calling Reshape  
  : capacity_(0) {  
  Reshape(shape);  
}  
//返回cpu 中的数据  
template &lt;typename Dtype&gt;  
const Dtype* Blob&lt;Dtype&gt;::cpu_data() const {  
  CHECK(data_);  
  return (const Dtype*)data_-&gt;cpu_data();  
}  
// 清空cpu 数据  
template &lt;typename Dtype&gt;  
void Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) {  
  CHECK(data);  
  data_-&gt;set_cpu_data(data);  
}  
//返回gpu 中的数据  
template &lt;typename Dtype&gt;  
const Dtype* Blob&lt;Dtype&gt;::gpu_data() const {  
  CHECK(data_);  
  return (const Dtype*)data_-&gt;gpu_data();  
}  
//反向传播导数diff_ 操作函数,返回cpu 中的数据  
template &lt;typename Dtype&gt;  
const Dtype* Blob&lt;Dtype&gt;::cpu_diff() const {  
  CHECK(diff_);  
  return (const Dtype*)diff_-&gt;cpu_data();  
}  
//返回gpu 中的数据  
template &lt;typename Dtype&gt;  
const Dtype* Blob&lt;Dtype&gt;::gpu_diff() const {  
  CHECK(diff_);  
  return (const Dtype*)diff_-&gt;gpu_data();  
}  

template &lt;typename Dtype&gt;  
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() {  
  CHECK(data_);  
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());  
}  

template &lt;typename Dtype&gt;  
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_data() {  
  CHECK(data_);  
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data());  
}  

template &lt;typename Dtype&gt;  
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_diff() {  
  CHECK(diff_);  
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data());  
}  

template &lt;typename Dtype&gt;  
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_diff() {  
  CHECK(diff_);  
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data());  
}  
//当前的blob 的data_ 指向已知blob的数据  
template &lt;typename Dtype&gt;  
void Blob&lt;Dtype&gt;::ShareData(const Blob&amp; other) {  
  CHECK_EQ(count_, other.count());  
  data_ = other.data();  
}  
//当前的blob 的diff_ 指向已知blob的反向传播导数  
template &lt;typename Dtype&gt;  
void Blob&lt;Dtype&gt;::ShareDiff(const Blob&amp; other) {  
  CHECK_EQ(count_, other.count());  
  diff_ = other.diff();  
}  

// The &quot;update&quot; method is used for parameter blobs in a Net, which are stored  
// as Blob&lt;float&gt; or Blob&lt;double&gt; -- hence we do not define it for  
// Blob&lt;int&gt; or Blob&lt;unsigned int&gt;.  
template &lt;&gt; void Blob&lt;unsigned int&gt;::Update() { NOT_IMPLEMENTED; }  
template &lt;&gt; void Blob&lt;int&gt;::Update() { NOT_IMPLEMENTED; }  
//Updata函数用于参数blob的更新（weight，bias 等减去对应的导数）  
template &lt;typename Dtype&gt;  
void Blob&lt;Dtype&gt;::Update() {  
  // We will perform update based on where the data is located.  
  switch (data_-&gt;head()) {  
  case SyncedMemory::HEAD_AT_CPU://数据在cpu上，则在cpu上进行计算  
    // perform computation on CPU  
    caffe_axpy&lt;Dtype&gt;(count_, Dtype(-1),  
        static_cast&lt;const Dtype*&gt;(diff_-&gt;cpu_data()),  
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));  
    break;  
  case SyncedMemory::HEAD_AT_GPU:  
  case SyncedMemory::SYNCED:  
#ifndef CPU_ONLY//如果没有定义CPU_ONLY，且数据在gpu上，则在gpu上进行计算  
    // perform computation on GPU  
    caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(-1),  
        static_cast&lt;const Dtype*&gt;(diff_-&gt;gpu_data()),  
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));  
#else  
    NO_GPU;  
#endif  
    break;  
  default:  
    LOG(FATAL) &lt;&lt; &quot;Syncedmem not initialized.&quot;;  
  }  
}  

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_data() const {  
  NOT_IMPLEMENTED;  
  return 0;  
}  

template &lt;&gt; int Blob&lt;int&gt;::asum_data() const {  
  NOT_IMPLEMENTED;  
  return 0;  
}  
//返回data_ 中所有 element 的绝对值之和  
template &lt;typename Dtype&gt;  
Dtype Blob&lt;Dtype&gt;::asum_data() const {  
  if (!data_) { return 0; }  
  switch (data_-&gt;head()) {  
  case SyncedMemory::HEAD_AT_CPU:  
    return caffe_cpu_asum(count_, cpu_data());  
  case SyncedMemory::HEAD_AT_GPU:  
  case SyncedMemory::SYNCED:  
#ifndef CPU_ONLY  
  {  
    Dtype asum;  
    caffe_gpu_asum(count_, gpu_data(), &amp;asum);  
    return asum;  
  }  
#else  
    NO_GPU;  
#endif  
  case SyncedMemory::UNINITIALIZED:  
    return 0;  
  default:  
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();  
  }  
  return 0;  
}  

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_diff() const {  
  NOT_IMPLEMENTED;  
  return 0;  
}  

template &lt;&gt; int Blob&lt;int&gt;::asum_diff() const {  
  NOT_IMPLEMENTED;  
  return 0;  
}  
//返回diff_ 中所有 element 的绝对值之和  
template &lt;typename Dtype&gt;  
Dtype Blob&lt;Dtype&gt;::asum_diff() const {  
  if (!diff_) { return 0; }  
  switch (diff_-&gt;head()) {  
  case SyncedMemory::HEAD_AT_CPU:  
    return caffe_cpu_asum(count_, cpu_diff());  
  case SyncedMemory::HEAD_AT_GPU:  
  case SyncedMemory::SYNCED:  
#ifndef CPU_ONLY  
  {  
    Dtype asum;  
    caffe_gpu_asum(count_, gpu_diff(), &amp;asum);  
    return asum;  
  }  
#else  
    NO_GPU;  
#endif  
  case SyncedMemory::UNINITIALIZED:  
    return 0;  
  default:  
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();  
  }  
  return 0;  
}  

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_data() const {  
  NOT_IMPLEMENTED;  
  return 0;  
}  

template &lt;&gt; int Blob&lt;int&gt;::sumsq_data() const {  
  NOT_IMPLEMENTED;  
  return 0;  
}  
//返回 data_ 中所有 element 的平方和  
template &lt;typename Dtype&gt;  
Dtype Blob&lt;Dtype&gt;::sumsq_data() const {  
  Dtype sumsq;  
  const Dtype* data;  
  if (!data_) { return 0; }  
  switch (data_-&gt;head()) {  
  case SyncedMemory::HEAD_AT_CPU:  
    data = cpu_data();  
    sumsq = caffe_cpu_dot(count_, data, data);  
    break;  
  case SyncedMemory::HEAD_AT_GPU:  
  case SyncedMemory::SYNCED:  
#ifndef CPU_ONLY  
    data = gpu_data();  
    caffe_gpu_dot(count_, data, data, &amp;sumsq);  
#else  
    NO_GPU;  
#endif  
    break;  
  case SyncedMemory::UNINITIALIZED:  
    return 0;  
  default:  
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();  
  }  
  return sumsq;  
}  

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_diff() const {  
  NOT_IMPLEMENTED;  
  return 0;  
}  

template &lt;&gt; int Blob&lt;int&gt;::sumsq_diff() const {  
  NOT_IMPLEMENTED;  
  return 0;  
}  
//返回 diff_ 中所有 element 的平方和  
template &lt;typename Dtype&gt;  
Dtype Blob&lt;Dtype&gt;::sumsq_diff() const {  
  Dtype sumsq;  
  const Dtype* diff;  
  if (!diff_) { return 0; }  
  switch (diff_-&gt;head()) {  
  case SyncedMemory::HEAD_AT_CPU:  
    diff = cpu_diff();  
    sumsq = caffe_cpu_dot(count_, diff, diff);  
    break;  
  case SyncedMemory::HEAD_AT_GPU:  
  case SyncedMemory::SYNCED:  
#ifndef CPU_ONLY  
    diff = gpu_diff();  
    caffe_gpu_dot(count_, diff, diff, &amp;sumsq);  
    break;  
#else  
    NO_GPU;  
#endif  
  case SyncedMemory::UNINITIALIZED:  
    return 0;  
  default:  
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();  
  }  
  return sumsq;  
}  

template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_data(unsigned int scale_factor) {  
  NOT_IMPLEMENTED;  
}  

template &lt;&gt; void Blob&lt;int&gt;::scale_data(int scale_factor) {  
  NOT_IMPLEMENTED;  
}  
// 给data乘以scale_factor  
template &lt;typename Dtype&gt;  
void Blob&lt;Dtype&gt;::scale_data(Dtype scale_factor) {  
  Dtype* data;  
  if (!data_) { return; }  
  switch (data_-&gt;head()) {  
  case SyncedMemory::HEAD_AT_CPU:  
    data = mutable_cpu_data();  
    caffe_scal(count_, scale_factor, data);  
    return;  
  case SyncedMemory::HEAD_AT_GPU:  
  case SyncedMemory::SYNCED:  
#ifndef CPU_ONLY  
    data = mutable_gpu_data();  
    caffe_gpu_scal(count_, scale_factor, data);  
    return;  
#else  
    NO_GPU;  
#endif  
  case SyncedMemory::UNINITIALIZED:  
    return;  
  default:  
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();  
  }  
}  

template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_diff(unsigned int scale_factor) {  
  NOT_IMPLEMENTED;  
}  

template &lt;&gt; void Blob&lt;int&gt;::scale_diff(int scale_factor) {  
  NOT_IMPLEMENTED;  
}  
// 给diff乘以scale_factor  
template &lt;typename Dtype&gt;  
void Blob&lt;Dtype&gt;::scale_diff(Dtype scale_factor) {  
  Dtype* diff;  
  if (!diff_) { return; }  
  switch (diff_-&gt;head()) {  
  case SyncedMemory::HEAD_AT_CPU:  
    diff = mutable_cpu_diff();  
    caffe_scal(count_, scale_factor, diff);  
    return;  
  case SyncedMemory::HEAD_AT_GPU:  
  case SyncedMemory::SYNCED:  
#ifndef CPU_ONLY  
    diff = mutable_gpu_diff();  
    caffe_gpu_scal(count_, scale_factor, diff);  
    return;  
#else  
    NO_GPU;  
#endif  
  case SyncedMemory::UNINITIALIZED:  
    return;  
  default:  
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();  
  }  
}  
//BlobProto 是定义在caffe.proto 中的一个message，其字段有 data,diff,shape,num,channels,height,width  
template &lt;typename Dtype&gt;  
bool Blob&lt;Dtype&gt;::ShapeEquals(const BlobProto&amp; other) {  
  if (other.has_num() || other.has_channels() ||  
      other.has_height() || other.has_width()) {  
    // Using deprecated 4D Blob dimensions --  
    // shape is (num, channels, height, width).  
    // Note: we do not use the normal Blob::num(), Blob::channels(), etc.  
    // methods as these index from the beginning of the blob shape, where legacy  
    // parameter blobs were indexed from the end of the blob shape (e.g., bias  
    // Blob shape (1 x 1 x 1 x N), IP layer weight Blob shape (1 x 1 x M x N)).  
    return shape_.size() &lt;= 4 &amp;&amp;  
           LegacyShape(-4) == other.num() &amp;&amp;  
           LegacyShape(-3) == other.channels() &amp;&amp;  
           LegacyShape(-2) == other.height() &amp;&amp;  
           LegacyShape(-1) == other.width();  
  }  
  vector&lt;int&gt; other_shape(other.shape().dim_size());  
  for (int i = 0; i &lt; other.shape().dim_size(); ++i) {  
    other_shape[i] = other.shape().dim(i);  
  }  
  return shape_ == other_shape;  
}//检查当前的blob和已知的 other 的 shape 是否相同，相同返回true  

template &lt;typename Dtype&gt;  
void Blob&lt;Dtype&gt;::CopyFrom(const Blob&amp; source, bool copy_diff, bool reshape) {  
  if (source.count() != count_ || source.shape() != shape_) {  
    if (reshape) {  
      ReshapeLike(source);  
    } else {  
      LOG(FATAL) &lt;&lt; &quot;Trying to copy blobs of different sizes.&quot;;  
    }  
  }  
  switch (Caffe::mode()) {  
  case Caffe::GPU:  
    if (copy_diff) {  
      caffe_copy(count_, source.gpu_diff(),  
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));  
    } else {  
      caffe_copy(count_, source.gpu_data(),  
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));  
    }  
    break;  
  case Caffe::CPU:  
    if (copy_diff) {  
      caffe_copy(count_, source.cpu_diff(),  
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));  
    } else {  
      caffe_copy(count_, source.cpu_data(),  
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));  
    }  
    break;  
  default:  
    LOG(FATAL) &lt;&lt; &quot;Unknown caffe mode.&quot;;  
  }  
}//从source 拷贝数据,copy_diff控制是拷贝diff还是data  

template &lt;typename Dtype&gt;  
void Blob&lt;Dtype&gt;::FromProto(const BlobProto&amp; proto, bool reshape) {  
  if (reshape) {  
    vector&lt;int&gt; shape;  
    if (proto.has_num() || proto.has_channels() ||  
        proto.has_height() || proto.has_width()) {  
      // Using deprecated 4D Blob dimensions --  
      // shape is (num, channels, height, width).  
      shape.resize(4);  
      shape[0] = proto.num();  
      shape[1] = proto.channels();  
      shape[2] = proto.height();  
      shape[3] = proto.width();  
    } else {  
      shape.resize(proto.shape().dim_size());  
      for (int i = 0; i &lt; proto.shape().dim_size(); ++i) {  
        shape[i] = proto.shape().dim(i);  
      }  
    }  
    Reshape(shape);  
  } else {//如果不做reshape要求当前的blob的shape和proto传入的shape相同  
    CHECK(ShapeEquals(proto)) &lt;&lt; &quot;shape mismatch (reshape not set)&quot;;  
  }  
  // copy data  
  Dtype* data_vec = mutable_cpu_data();  
  for (int i = 0; i &lt; count_; ++i) {  
    data_vec[i] = proto.data(i);  
  }//将proto传入的data拷贝到cpu数据  
  if (proto.diff_size() &gt; 0) {  
    Dtype* diff_vec = mutable_cpu_diff();  
    for (int i = 0; i &lt; count_; ++i) {  
      diff_vec[i] = proto.diff(i);  
    }//将proto传入的diff 拷贝到cpu数据  
  }  
}  

template &lt;typename Dtype&gt;  
void Blob&lt;Dtype&gt;::ToProto(BlobProto* proto, bool write_diff) const {  
  proto-&gt;clear_shape();  
  for (int i = 0; i &lt; shape_.size(); ++i) {  
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);  
  }  
  proto-&gt;clear_data();  
  proto-&gt;clear_diff();  
  const Dtype* data_vec = cpu_data();  
  for (int i = 0; i &lt; count_; ++i) {  
    proto-&gt;add_data(data_vec[i]);//将data写入proto  
  }  
  if (write_diff) {  
    const Dtype* diff_vec = cpu_diff();  
    for (int i = 0; i &lt; count_; ++i) {  
      proto-&gt;add_diff(diff_vec[i]);//将diff写入proto  
    }  
  }  
}  

INSTANTIATE_CLASS(Blob);  
template class Blob&lt;int&gt;;  
template class Blob&lt;unsigned int&gt;;  

}  // namespace caffe  
</code></pre>
      
    </div>
    
    <div>
      
        
      
    </div>

    <div>
      
        
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/caffe/" rel="tag">#caffe</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/08/30/caffe-proto文件介绍/" rel="next" title="caffe.proto文件介绍">
                <i class="fa fa-chevron-left"></i> caffe.proto文件介绍
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/08/30/layer分类介绍/" rel="prev" title="layer分类介绍">
                layer分类介绍 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2016/08/30/Blob-hpp和cpp文件解读/" data-title="Blob.hpp和cpp文件解读" data-url="http://magic93.cn/2016/08/30/Blob-hpp和cpp文件解读/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/default_avatar.jpg" alt="Siqi Xiang">
          <p class="site-author-name" itemprop="name">Siqi Xiang</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">23</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/sqxiang" target="_blank" title="github">
                  
                    <i class="fa fa-globe"></i>
                  
                  github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/1978268491" target="_blank" title="weibo">
                  
                    <i class="fa fa-globe"></i>
                  
                  weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <p class="post-toc-empty">This post does not have a Table of Contents</p>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Siqi Xiang</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"sqxiang"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  





  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("8DLu5uaqHg23PnzE4XOMzT4C-gzGzoHsz", "HnUpGtm2HfJiKUvs5qNdJXxd");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

</body>
</html>
