---
title: 机器学习算法介绍(三)--Adaboost
date: 2016-05-02 14:41:41
tags:
 - 机器学习
 - Adaboost
 - 弱分类器
categories: 机器学习
---
<blockquote class="blockquote-center">Adaboost</blockquote>
<!-- more -->
参考了几篇博客，发现大同小异，adaboost还是比较简单的。
## 基本原理
几个弱分类器组合成一个强分类器，是一个迭代算法。怎么迭代？前一个分类器分错的数据，给它加大权重放到下一个分类器继续训练。为什么这样？你要前面错了，我再着重考虑你训练一次，你还错，那这个分类器错误率就比较高，根据权重计算公式$\alpha_t=\frac{1}{2}In(\frac{1-\epsilon_t}{\epsilon_t})$，误差$\epsilon_t$越大，权重越小。
　 最开始是boosting,以前还有过bootstaping和bagging的方法，不过都是简单的组合弱分类器，adaboost强在哪?ada是自适应的意思，它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。这就是它流弊的地方。

## 算法流程
给定一个训练集${(x_1,y_1),(x_2,y_3)...(x_n,y_n)}$,其中$y\in \{-1,+1\}$,Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。
1.首先初始化训练数据集的权值分布(这里的权值与分类器权重不一样),一般用平均吧，
$$
D_1=(w_{11},w_{12},...w_{1n}),w_{1i}=\frac{1}{n},i=1,2,3..n
$$
2.$m=1,2,..M$，当训练了$m$次后的权值集合为$D_m$,使用此权值集合分类数据集，得到二分类器$G_m$(本质上是一个决策树)比如:(E是决策树的分割点)
$$
G_m=\{\begin{matrix}1,x>E \\
-1,x<=E\end{matrix}
$$
3.计算$G_m$在数据集上的误差率,
$$
e_m=P(G_m(x_i)\neq y_i)=\sum\limits_{i=1}^{n}w_{mi}I(G_m(x_i)\neq y_i)
$$
4.计算$G_m(x)$的系数，$a_m$表示$G_m(x)$在最终分类器中的重要程度：
$$
\alpha_m=\frac{1}{2}In(\frac{1-e_m}{e_m})
$$
这与前面提过的公式一样。由上述式子可知，$e_m <= 1/2$时，$a_m >= 0$，且$a_m$随着$e_m$的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。
5.更新训练数据集的权值集合$D_{m+1}$
$$
D_{m+1}=(w_{m+1\,1},w_{m+1\,2},...w_{m+1\,n}),\\w_{m+1\,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)),i=1,2,3..n
$$
其中$Z_m=\sum\limits_{i=1}^{n}w_{mi}exp(-\alpha_my_iG_m(x_i))$是一个规范化因子，为了让所有训练数据的权值概率和为1，而其中我们可以看出当训练数据被正确分类时$y_iG_m(x_i)=1$，分类错误时$y_iG_m(x_i)=-1$，再加上$\alpha_m$对所有训练数据的权值贡献相同，所以当数据分类错误时$w_{mi}exp(-\alpha_my_iG_m(x_i))>w_{mi}$，权值增大了，分类正确时相反权值减小。
6.构建最终的分类器
$$
f(x)=\sum\limits_{m=1}^{n}\alpha_mG_m(x)\\
G(x)=sign(f(x))
$$

## 例子
下面，给定下列训练样本，请用AdaBoost算法学习一个强分类器。
![ada1](/img/ada1.jpeg)
求解过程：初始化训练数据的权值分布，令每个权值$W_{1i} = 1/N = 0.1$，其中，N = 10，i = 1,2, ..., 10，然后分别对于m = 1,2,3, ...等值进行迭代。

迭代过程1：对于m=1，在权值分布为D1的训练数据上，阈值v取2.5时误差率最低，故基本分类器为：
![ada2](/img/ada2.jpeg)
从而可得G1(x)在训练数据集上的误差率$e_1=P(G_1(x_i)≠y_i) = 0.3$
然后计算G1的系数：
![ada3](/img/ada3.jpeg)
接着更新训练数据的权值分布：
![ada4](/img/ada4.jpeg)
最后得到各个数据的权值分布：
$D_2=(0.0715, 0.0715, 0.0715, 0.0715, 0.0715,\\ 0.0715, 0.1666, 0.1666, 0.1666, 0.0715)$，分类函数$f_1(x)=0.4236G_1(x)$，故最终得到的分类器$sign(f_1(x))$在训练数据集上有3个误分类点。

迭代过程2：对于m=2，在权值分布为D2的训练数据上，阈值v取8.5时误差率最低，故基本分类器为：
![ada5](/img/ada5.jpeg)
G2(x)在训练数据集上的误差率$e_2=P(G2(x_i)≠y_i) = 0.2143$

计算G2的系数：
![ada6](/img/ada6.jpeg)
更新训练数据的权值分布：
![ada4](/img/ada4.jpeg)
$D_3=(0.0455, 0.0455, 0.0455, 0.1667, 0.1667, 0.01667,\\ 0.1060, 0.1060, 0.1060, 0.0455)$
$f_2(x)=0.4236G_1(x) + 0.6496G_2(x)$

分类器$sign(f_2(x))$在训练数据集上有3个误分类点。

迭代过程3：对于m=3，在权值分布为D3的训练数据上，阈值v取5.5时误差率最低，故基本分类器为：
![ada7](/img/ada7.jpeg)
G3(x)在训练数据集上的误差率$e_3=P(G_3(x_i)≠y_i) = 0.1820$
计算G3的系数：
![ada8](/img/ada8.jpeg)
更新训练数据的权值分布：
![ada4](/img/ada4.jpeg)
$D_4=(0.125, 0.125, 0.125, 0.102, 0.102, 0.102, 0.065,\\ 0.065, 0.065, 0.125)，\\f_3(x)=0.4236G_1(x) + 0.6496G_2(x)+0.7514G_3(x)$，分类器$sign(f_3(x))$在训练数据集上有0个误分类点。
这也是最终的分类器。
下面再看一个图示：
![ada1](/img/ada1.png)
图中，“+”和“-”分别表示两种类别，在这个过程中，我们使用水平或者垂直的直线作为分类器，来进行分类。
第一步:
![ada2](/img/ada2.png)
根据分类的正确率，得到一个新的样本分布D2­，一个子分类器h1
其中划圈的样本表示被分错的。在右边的途中，比较大的“+”表示对该样本做了加权。
第二步:
![ada3](/img/ada3.png)
根据分类的正确率，得到一个新的样本分布D3，一个子分类器h2.
第三步:
![ada4](/img/ada4.png)
第四步，整合所有子分类器
![ada5](/img/ada5.png)

## 误差界
通过上面的例子可知，Adaboost在学习的过程中不断减少训练误差e，那这个误差界到底是多少呢？
事实上，adaboost 的训练误差的上界为：
![ada9](/img/ada9.jpeg)
下面，咱们来通过推导来证明下上述式子。
当$G(x_i)≠y_i$时，$y_i*f(x_i)<0$，因而$exp(-y_i*f(x_i))≥1$，因此前半部分得证。

关于后半部分，别忘了：
![ada10](/img/ada10.jpeg)
整个的推导过程如下：
![ada11](/img/ada11.jpeg)
这个结果说明，可以在每一轮选取适当的$G_m$使得$Z_m$最小，从而使训练误差下降最快。接着，咱们来继续求上述结果的上界。
对于二分类而言，有如下结果：
![ada12](/img/ada12.jpeg)
其中，![ada13](/img/ada13.jpeg)。

继续证明下这个结论。

由之前$Z_m$的定义式跟本节最开始得到的结论可知：
![ada14](/img/ada14.jpeg)
而这个不等式可先由$e^x$和$1-x$的开根号，在点x的泰勒展开式推出。
值得一提的是，如果取$\gamma 1, γ2…$ 的最大值，记做$γ$（显然，$γ≥γ_i>0，i=1,2,...m$），则对于所有$m$，有：
![ada15](/img/ada15.jpeg)
这个结论表明，AdaBoost的训练误差是以指数速率下降的。另外，AdaBoost算法不需要事先知道下界$γ$，AdaBoost具有自适应性，它能适应弱分类器各自的训练误差率 。
