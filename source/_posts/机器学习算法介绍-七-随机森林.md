---
title: 机器学习算法介绍(七)--随机森林
date: 2017-03-12 10:37:00
tags:
 - 机器学习
 - 随机森林
categories: 机器学习
---
<blockquote class="blockquote-center">随机森林</blockquote>
<!-- more -->
随机森林是一个最近比较火的算法，它有很多的优点：

 - 在数据集上表现良好
 - 在当前的很多数据集上，相对其他算法有着很大的优势 
 - 它能够处理很高维度（feature很多）的数据，并且不用做特征选择
 - 在训练完后，它能够给出哪些feature比较重要 
 - 在创建随机森林的时候，对generlization error使用的是无偏估计 
 - 训练速度快
 - 在训练过程中，能够检测到feature间的互相影响 
 - 容易做成并行化方法 
 - 实现比较简单
 
是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。
随机森林是一种多功能的机器学习算法，能够执行回归和分类的任务。同时，它也是一种数据降维手段，用于处理缺失值、异常值以及其他数据探索中的重要步骤，并取得了不错的成效。另外，它还担任了集成学习中的重要方法，在将几个低效模型整合为一个高效模型时大显身手。
在随机森林中，我们将生成很多的决策树，并不像在CART模型里一样只生成唯一的树。当在基于某些属性对一个新的对象进行分类判别时，随机森林中的每一棵树都会给出自己的分类选择，并由此进行“投票”，森林整体的输出结果将会是票数最多的分类选项；而在回归问题中，随机森林的输出将会是所有决策树输出的平均值。

随机森林在bagging的基础上还多了属性扰动，不止样本扰动，个体学习器之间的差异更大。bagging采用确定型决策树，而随机森林是随机型决策树。
[bagging与随机森林算法小结](http://blog.csdn.net/sun_shengyun/article/details/54616386)
GBDT的子采样是无放回采样，而Bagging的子采样是放回采样。
训练集中大约有36.8%的数据没有被采样集采集中。自助采样会改变数据的初始分布导致引入估计偏差。这部分数据一般称为包外数据，用来检测泛化能力
由于Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对于降低模型的方差很有作用。当然对于训练集的拟合程度就会差一些，也就是模型的偏倚会大一些。

RF使用了CART决策树作为弱学习器，这让我们想到了梯度提升树GBDT。第二，在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是RF通过随机选择节点上的一部分样本特征，这个数字小于n，假设为nsub，然后在这些随机选择的nsub个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。

## 缺点
1.对于有不同级别的属性的数据，级别划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的；
2.单棵决策树的预测效果很差：由于随机选择属性，使得单棵决策树的预测效果很差。
