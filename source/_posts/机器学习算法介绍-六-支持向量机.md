---
title: 机器学习算法介绍(六)--支持向量机
date: 2017-03-12 10:35:28
tags:
 - 机器学习
 - SVM
categories: 机器学习
---
<blockquote class="blockquote-center">支持向量机</blockquote>
<!-- more -->
线性分类器(一定意义上,也可以叫做感知机) 是最简单也很有效的分类器形式.在一个线性分类器中,可以看到SVM形成的思路,并接触很多SVM的核心概念.
 支持向量机（support vector　machines，SVM)是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming，不怕，附录有解释)的问题，也等价于正则化的合页损失函数（后面也有解释）的最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。 

## 线性可分支持向量机
 给定一个特征空间上的数据集$T=\{(x_1,y_1),(x_2,y_2)...(x_n,y_n)\}$
 一个线性分类器是$w^Tx+b=0$, $x_i\in R^n,y_i\in \{-1,1\}$
给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到分离超平面$wx+b=0$,以及分类决策函数$f(x)=sign(wx+b)$
$$f(x)=
\begin{cases}
-1& \text{wx+b<0}\\
1& \text{wx+b>0}
\end{cases}$$
![](/img/svm.jpeg)
说明：这里的$y\in \{-1,1\}$只是一种形式化标记而已，与逻辑回归里的$\{0,1\}$没有区别。
那如何确定w和b呢？答案是寻找两条边界端或极端划分直线中间的最大间隔
### 函数间隔与几何间隔
一个点距离分离超平面的远近可以表示分类预测的确信程度。$|w*x+b|$能够相对的表示点x到距离超平面的远近，因此我们可以定义超平面关于点$(x_i,y_i)$的函数间隔为:
$\hat{\gamma}_i=y_i(w*x_i+b)$
关于所有样本点的函数间隔为:
$\hat{\gamma}=min_{i=1,..,n}\hat{\gamma}_i$
上述定义的函数间隔虽然可以表示分类预测的正确性和确信度，但在选择分类超平面时，只有函数间隔还远远不够，因为如果成比例的改变w和b，如将他们改变为2w和2b，虽然此时超平面没有改变，但函数间隔的值f(x)却变成了原来的2倍。
我们可以对法向量$w$加些约束条件，使其表面上看起来规范化，如此，我们很快又将引出真正定义点到超平面的距离--几何间隔的概念
我们可以定义超平面关于点$(x_i,y_i)$的几何间隔为:
${\gamma}_i=y_i(\frac{w}{||w||}*x_i+\frac{b}{||w||})$
关于所有样本点的几何间隔为:
${\gamma}=min_{i=1,..,n}{\gamma}_i$

### 间隔最大化
直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。
如何最大化间隔，可以转换成最大化几何间隔，也就是以下约束问题：
$max_{w,b}\gamma$
$s.t. y_i(\frac{w}{||w||}*x_i+\frac{b}{||w||})\geq\gamma, i=1,2,...,N$
等价于:
$max_{w,b}\frac{\hat{\gamma}}{||w||}$
$s.t. y_i({w}*x_i+{b})\geq\hat\gamma, i=1,2,...,N$
函数间隔改变对不等式优化没有影响，可以取$\hat{\gamma}=1$
等价于:
$min_{w,b} \frac{1}{2}{||w||}^2$
$s.t.   y_i(wx_i+b)-1\geq0, i=1,2...,N$

### 支持向量
距离分离超平面最近的点称为支持向量，只有它起作用，间隔为$\frac{2}{||w||}$
![](/img/svm_1.png)
可以看到两个支撑着中间的 gap 的超平面，它们到中间的纯红线separating hyper plane 的距离相等，即我们所能得到的最大的$\gamma$，而“支撑”这两个超平面的必定会有一些点，而这些“支撑”的点便叫做支持向量Support Vector
 
### 对偶算法
对偶问题更容易求解，可以自然的引入核函数
引入拉格朗日乘子后函数为:
$L(w,b,\alpha)=\frac{1}{2}{||w||}^2-\sum_{i=1}^{N}\alpha_iy_i(wx_i+b)+\sum_{i=1}^{N}\alpha_i$
然后我们令:
$\theta(w)=max_{\alpha_i\geq0}L(w,b,\alpha)$
当约束条件不满足时，此时最大值为无穷大，因此满足时，最大值为$\frac{1}{2}{||w||}^2$,因此原问题等价于:
$min_{w,b}max_{\alpha_i\geq0}L(w,b,\alpha)$
对偶问题为:
$$max_{\alpha_i\geq0}min_{w,b}L(w,b,\alpha)$$
满足KKT条件时两者等价，为什么满足，看机器学习的书
选择求$L$对$w,b$的偏导，有:
$w=\sum_{i=1}^{N}\alpha_iy_ix_i$
$\sum_{i=1}^{N}\alpha_iy_i=0$
代会原式得:
$min_{w,b}L(w,b,\alpha)=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_ix_j)+\sum_{i=1}^{N}\alpha_i$
再求该式子的极大，等价于:
$min_{\alpha}\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_ix_j)-\sum_{i=1}^{N}\alpha_i$
$s.t. \sum_{i=1}^{N}\alpha_iy_i=0$
$\alpha_i\geq0, i=1,2..N$
根据这个式子求出$\alpha$,就可以得到:
$w=\sum_{i=1}^{N}\alpha_iy_ix_i$
并根据$\alpha$中的一个大于０的分量$\alpha_j$得到：
$b=y_j-wx_j=y_j-\sum_{i=1}^{N}\alpha_iy_i(x_ix_j)$
$(x_j,y_j)$为支持向量
由式子:
$L(w,b,\alpha)=\frac{1}{2}{||w||}^2-\sum_{i=1}^{N}\alpha_iy_i(wx_i+b)+\sum_{i=1}^{N}\alpha_i=\\
\frac{1}{2}{||w||}^2-\sum_{i=1}^{N}\alpha_i(y_i(wx_i+b)-1)$
注意到如果 $x_j$ 是支持向量的话，上式中后半部分是等于 0 的（因为支持向量的 functional margin 等于 1 ），而对于非支持向量来说，functional margin 会大于 1 ，因此后半部分是大于零的，而$\alpha$又是非负的，为了满足最大化，必须等于 0 。这也就是这些非Supporting Vector 的点的局限性。
 
## 线性支持向量机与软间隔
原始问题：
$min_{w,b,\xi} \frac{1}{2}{||w||}^2+C\sum_{i=1}^{N}{\xi}_i$
$s.t. y_i(wx_i+b)\geq1-\xi_i, i=1,2...N$
$\xi_i\geq0,i=1,2...N$
对偶问题:
$min_{\alpha}\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_ix_j)-\sum_{i=1}^{N}\alpha_i$
$s.t. \sum_{i=1}^{N}\alpha_iy_i=0$
$0\leq\alpha_i\leq{C}, i=1,2,3...N$

## 非线性支持向量机与核函数
将非线性问题转换为线性问题一般用一个非线性变换，$z=\phi(x)$,一般从低维映射到高维，但一股脑的映射过去会造成无穷维度的问题，因此我们可以采用核函数的方法，
$K(x,z)=\phi(x)\phi(z)$
区别在于：
  一个是映射到高维空间中，然后再根据内积的公式进行计算；
而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。
映射后分类函数为:
$\sum_{i=1}^{N}\alpha_iy_iK(x_i,x)+b=0$
$\alpha_i$是通过求解下面的式子求出的：
$min_{\alpha}\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_jK(x_ix_j)-\sum_{i=1}^{N}\alpha_i$
$s.t. \sum_{i=1}^{N}\alpha_iy_i=0$
$0\leq\alpha_i\leq{C}, i=1,2,3...N$

有多项式核函数，高斯核函数，字符核函数等
$$y_i(\sum_{j=1}^{N}\alpha_jy_jK(x_j,x_i))=
\begin{cases}
\geq1& \text{$\alpha_i$=0}\\
=1& \text{0<$\alpha_i$<C}\\
\leq1& \text{$\alpha_i$=C}
\end{cases}$$

## SMO算法
参考：[SMO算法](http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html)

 参考：[支持向量机](http://www.ppvke.com/Blog/archives/24242)
