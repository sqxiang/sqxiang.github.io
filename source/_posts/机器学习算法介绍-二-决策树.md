---
title: 机器学习算法介绍(二)--决策树
date: 2016-04-25 14:21:34
tags:
 - 机器学习
 - 决策树
 - CART
 - ID3
 - C4.5
categories: 机器学习
---
<blockquote class="blockquote-center">决策树</blockquote>
<!-- more -->
据听说目前市面上流行的算法还不是神经网络，因此又不得不去看一些工业算法啊。累挺～。
## 信息论的一点东西
### 1.熵
一个信源发送出什么符号是不确定的，衡量它可以根据其出现的概率来度量。概率大，出现机会多，不确定性小；反之就大。
设随机变量$X$有$k$种取值，对应每种取值有一个概率$p_{i}$,我们定义一个不确定性函数，得满足两点:
 1. 是概率的递减函数，即概率越大，不确定性越小
 2. 满足可加性，即$f(p_{i}p_{j}) = f(p_{i})+f(p_{j})$，即联合不确定性是两者之和
 这样的函数很容(ji)易(zhi)想到:$f(p_{i})=-log(p_{i})$。ok,有了函数就可以定义变量$X$的不确定性，也就是熵了，它应该是取每个值不确定性的统计平均，也就是:$H(X)=E(-log(p))=-\sum\limits_{i=1}^{k}p_{i}log(p_{i})$
式中对数一般取2为底，单位为比特。但是，也可以取其它对数底，采用其它相应的单位，它们间可用换底公式换算。

### 2.KL divergence(KL距离)
又被称为相对熵，信息增益，信息散度。
这个值是用来衡量两个分布之间相异度的，KL散度是两个概率分布P和Q差别的非对称性的度量。 KL散度是用来度量使用基于Q的编码来编码来自P的样本平均所需的额外的比特个数。典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。具体来说，假设有k个状态的两个离散分布p,q，则:
$$
\begin{eqnarray}
KL(p||q)&=&\sum\limits_{i=1}^{k}p_{i}log\frac{p_{i}}{q_{i}}\\
 &=& \sum\limits_{i=1}^{k}p_{i}log(p_{i})-\sum\limits_{i=1}^{k}p_{i}log(q_{i})\\
 &=&H(p,q)-H(p)
\end{eqnarray}
$$
如果是连续的分布p,q，则KL散度为:$KL(p||q)=\int p_{i}log\frac{p_{i}}{q_{i}}di$
 KL散度表示当用概率分布q来拟合真实分布p时，产生的信息损耗，其中p表示真实分布，q表示p的拟合分布。不具有对称性，其中$H(p,q)$也被成为交叉熵。交叉熵可以看作是当我们用模型 q来编码来自模型p的变量时所需的平均bits(如果log以2为底的话)
所以，有H(p)=H(p,p),所以KL距离就可以看做是：用模型q来编码来自模型p的变量所需的额外bits！    $KL(p||q)>=0$
理解:
$H(X)$是变量$X$的熵，也就是变量取值越多，变化越多，其熵也就越大，KL距离其实描述的是一种差异，是一种样本观察(p)和理论上的(q)差异。

### 3.互信息
我们衡量两个随机变量的$X,Y$相关性时，要考虑$X,Y$的联合概率$P(X,Y)$和各自的概率$P(X),P(Y)$,互信息就是联合分布$P(X,Y)$与乘积$P(X)P(Y)$的相对熵。
$$
I(X;Y)=KL(P(X,Y)||P(X)P(Y))=\sum\limits_{x}\sum\limits_{y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}
$$
这个式子有很多理解:

 - 当$X,Y$不相关时，$p(x,y)=p(x)p(y)$，互信息为０，
 - 根据熵的连锁规律$H(X,Y) = H(X)+H(Y|X)=H(Y)+H(X|Y)$,因此互信息还有一种表示$I(X,Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)$这个式子由原式也可以推导出来。
 - 互信息和KL散度不一样，它是对称的，即$I(X,Y)=I(Y,X)$
 - 掷一次骰子，由于六种结局（点）的出现概率相等，所以结局的不确定程度（熵）为log6 ，如果告诉你掷骰子的结局是单数或者双数，这显然是一个信息。这个信息消除了我们的一些不确定性。把消除的不确定性称为互信息显然是妥当的。$H(X|Y)$就是当知道点数是单还是双之后的不确定性(熵)，必然是减小了。
 
### ４．文本分类问题
讲了这么多概念，我们当然是希望知道这些理论的用处。
有一个假设：在某个特定类别出现频率高,但在其他类别出现频率比较低的词条与该类的互信息比较大。通常用互信息作为特征词和类别之间的测度，如果特征词属于该类的话，它们的互信息量最大。
所以互信息可以衡量特征与类别之间的相关度。
对于文本分类或聚类而言，就是说文档属于哪个类别的变化越多，类别的信息量就越大。所以特征T给聚类C或分类C带来的信息增益为IG(T)=H(C)-H(C|T)。这里信息增益与互信息概念相通。
**I(T,C) = log( P(T|C)/P(T) )**
**I(T,C) = H(C)-H(C|T)**
其中P(T｜C)是词条T在类别C中出现的概率，P(T)是词条T在整个训练集中出现的概率。

直观理解， 如果 T在整个训练集中出现的概率并不高，但是在类别C中出现的概率却很高， 那么就可以知道T可以很大程度上代表这个C, 分子类似于tf（但是包含了文档长度的考虑）, 分母有点类似于IDF（但是又包含了各个文档中出现的term的个数，而不只是出没出现）。
其中T的取值若有$k$种，$H(C|T)=\sum p(T_{i})(H(C|T_{i}))$
用互信息的方法，在某个类别C中的出现概率高，而在其它类别中的出现概率低的词条T，将获得较高的词条和类别互信息，也就可能被选取为类别C的特征。

互信息是term的存在与否能给类别c的正确判断带来的信息量。

词条和类别的互信息体现了词条和类别的相关程度，互信息越大，词条和类别的相关程度也越大。得到词条和类别之间的相关程度后，选取一定比例的，排名靠前的词条作为最能代表此种类别的特征。

在信息增益中，衡量标准是看特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。对一个特征而言，系统有它和没它时信息量将发生变化，而前后信息量的差值就是这个特征给系统带来的信息量。所谓信息量，就是熵。
注意，这里特征是相对于整个系统而言。

### ５．信息增益
刚才也提到了信息增益，之所以单拎出来再讲一遍，是因为这里的概念相通性太高，把概念比喻成集合的话，应该是信息增益$\subseteq$互信息$\subseteq$KL散度，不确定性都用熵来衡量，而信息增益最完美的解释应该是**熵降低的程度。**
也就是在引入某个特征$T$,或者某个变量$X$后，原始变量$Y$的不确定性减小的程度。
$H(X,Y)-H(X)=H(Y|X),H(Y)-H(Y|X)=I(X,Y)$
第一个式子就是一般的KL散度，也就是如果已经完全知道第二个随机变量 X 的前提下，随机变量 Y 的信息熵还有多少。也就是 基于 X 的 Y 的信息熵。其实也是一种信息增益（很晕，很绕），第二个式子就是真正的信息增益了。它用$Y$本来的熵减去知道$X$之后$Y$还剩下的熵，得到的就是减少的不确定度，也就是增加的信息量了。

参考博客[熵、KL散度、信息增益、互信息-学习笔记](https://segmentfault.com/a/1190000000641079),[信息论(Information theory)的一些point ](http://blog.csdn.net/dark_scope/article/details/8459576)以及各种百度百科

## 决策树
如果你上面的知识弄懂的差不多了的话，决策树就显得很简单了。
### 什么是决策树
决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。
这是理论，我们来举个例子，这个例子来源于Tom M.Mitchell著的机器学习一书。
  小王的目的是通过下周天气预报寻找什么时候人们会打高尔夫，他了解到人们决定是否打球的原因最主要取决于天气情况。而天气状况有晴，云和雨；气温用华氏温度表示；相对湿度用百分比；还有有无风。如此，我们便可以构造一棵决策树，如下（根据天气这个分类决策这天是否合适打网球）：
  ![no](/img/jueceshu.jpeg)
  第二个例子:
  策树分类的思想类似于找对象。现想象一个女孩的母亲要给这个女孩介绍男朋友，于是有了下面的对话：

      女儿：多大年纪了？
      母亲：26。
      女儿：长的帅不帅？
      母亲：挺帅的。
      女儿：收入高不？
      母亲：不算很高，中等情况。
      女儿：是公务员不？
      母亲：是，在税务局上班呢。
      女儿：那好，我去见见。

　　这个女孩的决策过程就是典型的分类树决策。相当于通过年龄、长相、收入和是否公务员对将男人分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么这个可以用下图表示女孩的决策逻辑：
　　![error](/img/jueceshu2.jpeg)
　　
### 如何构建决策树
决策树的基本步骤如下:

 1. 所有记录看做根节点
 2. 对每个特征的每种分割遍历，找到最好的分割方式
 3. 如果是二叉树，则要考虑每种特征的二分策略，如不是就按照不同特征划分
 4. 用某种方式（信息增益，信息增益率，基尼系数)计算出每个特征划分的"纯度"，利用最高"纯度"的特征进行第一次划分
 5. 划分成两个或多个叶子节点，剔除用过的特征，重复第4步
 6. 划分直到所有叶子节点只包含一种分类(纯度最高),划分结束
 7. 如果特征用完还未划分结束，则最后一次根据叶子节点中最多数的某个类别进行划分，划分结束。
 
构造决策树的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。
属性选择度量算法有很多，一般使用自顶向下递归分治法，并采用不回溯的贪心策略。这里介绍ID3和C4.5两种常用算法。

### ID3算法
一般选用信息增益来度量纯度，由上面的基础知识我们知道，信息增益越大，其特征给予全局的不确定度降低也就越大，也就意味着得知这个特征后更容易知道样本的类别，即纯度得到了提高。因此我们用贪心思想的话，第一次划分要选用信息增益最大的特征。
设$D$为训练集大小，$D_{i}$为第$i$类，共$k$类，$p_{i}$是分类$i$的概率,设第一次按照特征$A$进行划分,$A_{i}$为第$i$个划分个数，共$v$个划分，则$\sum\limits_{i=1}^{v}A_{i}=D$
这里的信息增益表示为:
$I=H(D)-H(D|A) =-\sum p(D_{i})log(p(D_{i}))+\sum p(A_{i})H(D|A_{i})\\= -\sum\limits_{i=1}^{k}p_{i}log(p_{i})+\sum\limits_{i=1}^{v}\frac{A_{i}}{D}p(D|A_{i})log(p(D|A_{i}))$
举个例子:
![table](/img/table.png)
其中s、m和l分别表示小、中和大。
设L、F、H和R表示日志密度、好友密度、是否使用真实头像和账号是否真实，下面计算各属性的信息增益。
$$
\begin{equation}
H(D)=-0.7log0.7-0.3log0.3=0.879\\
H(D|L) = 0.3(-\frac{0}{3}log\frac{0}{3}-\frac{3}{3}log\frac{3}{3})+0.4(-\frac{1}{4}log\frac{1}{4}-\frac{3}{4}log\frac{3}{4})\\+0.3(-\frac{1}{3}log\frac{1}{3}-\frac{2}{3}log\frac{2}{3})=0.603\\
I(D,L)=H(D)-H(D|L)=0.276
\end{equation}
$$
因此日志密度的信息增益是0.276。用同样方法得到H和F的信息增益分别为0.033和0.553。
因为F具有最大的信息增益，所以第一次分裂选择F为分裂属性，分裂后的结果如下图表示：
![table2](/img/table2.png)
由于$l,m$对应的类别都只有一种，因此到达叶子结点，划分结束，而$S$还包含了两种分类，因此要继续调用算法。
这里考虑的变量都是离散的，若变量连续，则元素按照特征属性排序，每两个相邻元素的中间点可以看做潜在分裂点，从第一个潜在分裂点开始，分裂D并计算两个集合的期望信息，具有最小期望信息的点称为这个属性的最佳分裂点，其信息期望作为此属性的信息期望。(信息期望就是H(D|T),T为特征划分)

### C4.5算法
 ID3算法存在一个问题，就是偏向于多值属性，例如，如果存在唯一标识属性ID，则ID3会选择它作为分裂属性，这样虽然使得划分充分纯净，但这种划分对分类几乎毫无用处。ID3的后继算法C4.5使用增益率（gain ratio）的信息增益扩充，试图克服这个偏倚。如一个训练集中有10个元组，对于某一个属相A，它分别取1-10这十个数，如果对A进行分裂将会分成10个类，那么对于每一个类$H(D|A_{i})=0$，从而$H(D|A)$为0，该属性划分所得到的信息增益$I$最大，但是很显然，这种划分没有意义。
 因此C4.5使用信息增益率将信息增益规范化，它使用了一个“分裂信息”，注意与ID3中$H(D)$的区别。我们把分裂信息记为$S(D,A)$,则$S(D,A)=\sum\limits_{i=1}^{v}\frac{A_{i}}{D}log(\frac{A_{i}}{D})$,这表示用属性$A$对整个训练集$D$的划分。
 信息增益率为$GainRatio(A)=\frac{I(D,A)}{S(D,A)}$
 C4.5以信息增益率作为分裂属性的度量，其他与$ID3$相似，不再赘述。
 
#### 补充
##### 如果属性用完了怎么办
在决策树构造过程中可能会出现这种情况：所有属性都作为分裂属性用光了，但有的子集还不是纯净集，即集合内的元素不属于同一类别。在这种情况下，由于没有更多信息可以使用了，一般对这些子集进行“多数表决”，即使用此子集中出现次数最多的类别作为此节点类别，然后将此节点作为叶子节点。
##### 关于剪枝
在实际构造决策树时，通常要进行剪枝，这时为了处理由于数据中的噪声和离群点导致的过分拟合问题。剪枝有两种：
先剪枝——在构造过程中，当某个节点满足剪枝条件，则直接停止此分支的构造。C4.5属于这种剪枝。
后剪枝——先构造完成完整的决策树，再通过某些条件遍历树进行剪枝。
C4.5采用悲观剪枝法，它使用训练集生成决策树又用它来进行剪枝，不需要独立的剪枝集。

    悲观剪枝法的基本思路是：设训练集生成的决策树是T，用T来分类训练集中的N的元组，设K为到达某个叶子节点的元组个数，其中分类错误地个数为J。由于树T是由训练集生成的，是适合训练集的，因此J/K不能可信地估计错误率。所以用(J+0.5)/K来表示。设S为T的子树，其叶节点个数为L(s)， 
$\sum K$为到达此子树的叶节点的元组个数总和， $\sum J$为此子树中被错误分类的元组个数之和。在分类新的元组时，则其错误分类个数为$\sum J + \frac{L(s)}{2}$，其标准错误表示为：$S_{e}(E)=\sqrt \frac{E\times (N-E)}{N}$ 。当用此树分类训练集时，设E为分类错误个数，当下面的式子成立时，则删掉子树S，用叶节点代替，且S的子树不必再计算。
    $E+\frac{1}{2}\leq \sum J +L(S)/2+S_{e}(\sum J +L(S)/2)$
### CART算法(分类回归树)
CART分裂属性选择的度量用的是GINI指数。它建立的是一颗二叉树，不管特征有多少取值。
GINI指数的定义如下:
$Gini(D)=1-\sum\limits_{i=1}^{m}p_{i}^{2}$
其中$p_{i}$表示属于$i$类的概率。
那么划分的过程中就是选择GINI指数最小的属性进行划分，同时同一属性中的不同取值要递归进行计算，划分点选择两个连续变量之间的值。
举个例子:
![cart1](/img/cart1.png)
在上述图中，属性有3个，分别是有房情况，婚姻状况和年收入，其中有房情况和婚姻状况是离散的取值，而年收入是连续的取值。拖欠贷款者属于分类的结果。
对于有房无房来说，属性是二分的，不必再划分，GINI指数计算如下:
![cart2](/img/cart2.png)
对于婚姻状况，有三种划分情况:
![cart3](/img/cart3.jpg)
最后年收入取值是连续的，要在每两个连续值中间找划分:
![cart6](/img/cart6.png)
找到最小的GINI指数，按照这个属性的这个值进行二分，这里可以选择婚姻的单身或离异与已婚，也可以选择年收入97左右进行划分。
**那么终止条件又是什么呢?**
直观的情况，当节点包含的数据记录都属于同一个类别时就可以终止分裂了。这只是一个特例，更一般的情况我们计算$\chi^{2}$值来判断分类条件和类别的相关程度，当$\chi^{2}$很小时说明分类条件和类别是独立的，即按照该分类条件进行分类是没有道理的，此时节点停止分裂。注意这里的“分类条件”是指按照GINI_Gain最小原则得到的“分类条件”。（先判断最小GINI的分类条件，然后再看这么分类的$\chi^{2}$值是否小于阈值，小于则停止划分)
[CART分类算法 ](http://blog.csdn.net/tianguokaka/article/details/9018933)这篇讲了如何看$\chi^{2}$
还有一种方式就是，如果某一分支覆盖的样本的个数如果小于一个阈值，那么也可产生叶子节点，从而终止Tree-Growth。
**如何确定叶子节点的类？**
前面提到Tree-Growth终止的方式有2种，对于第一种方式，叶子节点覆盖的样本都属于同一类，那么这种情况下叶子节点的类自然不必多言。对于第二种方式，叶子节点覆盖的样本未必属于同一类，直接一点的方法就是，该叶子节点所覆盖的样本哪个类占大多数，那么该叶子节点的类别就是那个占大多数的类。

### 剪枝
前面我们讨论过剪枝有两种方式，前向剪枝和后向剪枝，C4.5一般是后向剪枝中的悲观误差剪枝法。而CART采用的后向剪枝有如下几种:

 - 代价复杂性剪枝
 - 最小误差剪枝
 - 悲观误差剪枝
 
**代价复杂剪枝Cost-Complexity Pruning(CCP、代价复杂度)**
当分类回归树划分得太细时，会对噪声数据产生过拟合作用。因此我们要通过剪枝来解决。剪枝又分为前剪枝和后剪枝：前剪枝是指在构造树的过程中就知道哪些节点可以剪掉，于是干脆不对这些节点进行分裂，在N皇后问题和背包问题中用的都是前剪枝，上面的χ2方法也可以认为是一种前剪枝；后剪枝是指构造出完整的决策树之后再来考查哪些子树可以剪掉。
代价复杂剪枝过程:
对于分类回归树中的每一个非叶子节点计算它的表面误差率增益值$\alpha$。
$\alpha=\frac{R(t)-R(T_{t})}{N_{T_{t}}-1}$
$N_{T_{t}}$是子树中包含的叶子节点个数;
$R(t)$是节点t的误差代价，如果该节点被剪枝;
$R(t)=r(t)*p(t)$
$r(t)$是节点t的误差率;
$p(t)$是节点t上的数据占所有数据的比例。
$R(T_{t})$是子树$T_{t}$的误差代价，如果该节点不被剪枝。它等于子树$T_{t}$上所有叶子节点的误差代价之和。
比如有个非叶子节点$T_4$如图所示：
![jianzhi](/img/jianzhi.png)
已知所有的数据总共有60条，则节点t4的节点误差代价为：
![jianzhi2](/img/jianzhi2.gif)
子树误差代价是:
![jianzhi3](/img/jianzhi3.gif)
以$T_4$为根节点的子树上叶子节点有3个，最终：
![jianzhi4](/img/jianzhi4.gif)
找到$\alpha$值最小的非叶子节点，令其左右孩子为NULL。当多个非叶子节点的$\alpha$值同时达到最小时，取最大的进行剪枝。

**最小误差剪枝**
**Reduced-Error Pruning(REP,错误率降低剪枝）**
上面是同样的东西不同叫法。
该剪枝方法考虑将书上的每个节点作为修剪的候选对象，决定是否修剪这个结点有如下步骤组成：
1：删除以此结点为根的子树
2：使其成为叶子结点
3：赋予该结点关联的训练数据的最常见分类
4：当修剪后的树对于验证集合的性能不会比原来的树差时，才真正删除该结点
因为训练集合的过拟合，使得验证集合数据能够对其进行修正，反复进行上面的操作，从底向上的处理结点，删除那些能够最大限度的提高验证集合的精度的结点，直到进一步修剪有害为止(有害是指修剪会减低验证集合的精度)
REP是最简单的后剪枝方法之一，不过在数据量比较少的情况下，REP方法趋于过拟合而较少使用。这是因为训练数据集合中的特性在剪枝过程中被忽略，所以在验证数据集合比训练数据集合小的多时，要注意这个问题。
尽管REP有这个缺点，不过REP仍然作为一种基准来评价其它剪枝算法的性能。它对于两阶段决策树学习方法的优点和缺点提供了了一个很好的学习思路。由于验证集合没有参与决策树的创建，所以用REP剪枝后的决策树对于测试样例的偏差要好很多，能够解决一定程度的过拟合问题。 

**Pessimistic Error Pruning(PEP，悲观剪枝）**
先计算规则在它应用的训练样例上的精度，然后假定此估计精度为二项式分布，并计算它的标准差。对于给定的置信区间，采用下界估计作为规则性能的度量。这样做的结果，是对于大的数据集合，该剪枝策略能够非常接近观察精度，随着数据集合的减小，离观察精度越来越远。该剪枝方法尽管不是统计有效的，但是在实践中有效。
PEP为了提高对测试集合的预测可靠性，PEP对误差估计增加了连续性校正(Continuity Correction)。PEP方法认为，如果：
${e}'(t)\leq{e}'(T_{t})+S_{e}({e}'(T_{t}))$
成立，则Tt应该被剪枝，
上式中：
${e}'(t)=e(t)+1\\
{e}'(T_{t})=\sum e(i)+\frac{N_{t}}{2}$

其中，$e(t)$为结点t出的误差；i为覆盖$T_{t}$的叶子结点；$N_{t}$为子树$T_{t}$的叶子树；n(t)为在结点t处的训练集合数量。$S_{e}(E)=\sqrt \frac{E\times (N-E)}{N}$,E是错误个数，N是总个数。PEP采用自顶向下的方式，如果某个非叶子结点符合上面的不等式，就裁剪掉该叶子结点。该算法被认为是当前决策树后剪枝算法中经度比较高的算法之一，但是饿存在有缺陷。首先，PEP算法是唯一使用Top-Down剪枝策略，这种策略会导致与先剪枝出现同样的问题，将该结点的某子节点不需要被剪枝时被剪掉；另外PEP方法会有剪枝失败的情况出现。
虽然PEP方法存在一些局限性，但是在实际应用中表现出了较高的精度,。两外PEP方法不需要分离训练集合和验证机和，对于数据量比较少的情况比较有利。再者其剪枝策略比其它方法相比效率更高，速度更快。因为在剪枝过程中，树中的每颗子树最多需要访问一次，在最坏的情况下，它的计算时间复杂度也只和非剪枝树的非叶子节点数目成线性关系。
**(待续)**
**Adaboost
boosting
gbdt(gradient boosting decision tree)**

#### 参考博客
[数据挖掘十大算法之—C4.5](http://www.cnblogs.com/superhuake/archive/2012/07/25/2609124.html)
[决策树学习笔记整理](http://www.cnblogs.com/bourneli/archive/2013/03/15/2961568.html)
[分类算法-----决策树  ](http://blog.163.com/zhoulili1987619@126/blog/static/353082012013113083417956/)
[CART分类算法 ](http://blog.csdn.net/tianguokaka/article/details/9018933)
[数据挖掘十大经典算法--CART: 分类与回归树](http://www.tuicool.com/articles/jAB7ve)
[决策树之CART算法](http://blog.csdn.net/acdreamers/article/details/44664481)
[决策树--从原理到实现 ](http://blog.csdn.net/dark_scope/article/details/13168827)
[算法杂货铺——分类算法之决策树(Decision tree)](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)



